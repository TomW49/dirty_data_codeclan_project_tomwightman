---
title: "Documentation: clean_halloween_candy_2015_to_2017"
output:
  html_document: 
    number_sections: no
    toc: yes
    df_print: paged
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

By Tom Wightman

# Introduction

The `clean_halloween_candy_2015_to_2017.csv` data set is comprised of three original data sets:

-   `boing-boing-candy-2015.xlsx`
-   `boing-boing-candy-2016.xlsx`
-   `boing-boing-candy-2017.xlsx`

These data sets are based on collected data from annual surveys between 2015 and 2017 using Googlesheets, and what looks like an organisations internal survey, gaining responses from just under 10,000 people over the time period.

The surveys them self ask the users to rate their opinion on several "candy" types using a rating of either "JOY", "MEH" or "DESPAIR", however there are a number of discrepancies between each data set. In 2015, the "MEH" response was missing for the users, in addition to spelling errors leading to duplicate inputs - standardization was not a high priority of the creators!

More information of the data sets can be found [here](https://www.scq.ubc.ca/so-much-candy-data-seriously/).

# Assumptions

Some of the assumptions I made during the cleaning and analysis phase of the data are as follows:

-   The lowest age to be included in the data was 4, as I thought that parents/guardians could theoretically ask them if they like, really like, or hate specific candy. In reality however I doubt this would ever be the case, as a 4 year old would hopefully never be exposed to that much candy in their short lifespan.
-   The highest age to be included in the data was 100, as this seems now to be a reasonable age to reach, and still be able to answer a list of questions about candy - my granny actually lived to 101!
-   I also included NA's within the data sets, as this could prove to be useful data - a lot of NA's had additional data to them, so removing them would have taken away a lot of information.
-   I had removed all non-edible items, including medication, as I wanted a smaller data set for potential returns in runtimes and a cleaner look. I did this based on the questions presented to me, all specific about "candy" and "candy bars", not "what is your favourite font" and "vicodin" (paracetamol). I do recognise that "cash or other forms of legal tender" is a fair item to be given during Halloween (maybe in a wealthy neighborhood), however again using the questions as basis for cleaning I removed it.

# Cleaning

### Reading in the data

I used `tidyverse` and `janitor` packages to read in and clean the variable names in addition to using `here` package to aid reproducability.

### Cleaning `boing-boing-candy-2015.xlsx`

I used the below code to remove the "timestamp" variable and replace it with the year in order to identify when joining all three data sets. I also renamed two columns in order to better accommodate the join.

```{r, eval = FALSE}
pivot_prep_candy15 <- bbcandy2015 %>%   
  select(-timestamp) %>%    
  rename(going_out = are_you_going_actually_going_trick_or_treating_yourself, 
               age = how_old_are_you) %>%      
  add_column(year  = 2015, .before = 1)
```

I then pivoted the prepared data above using: 
```{r, eval = FALSE}
pivot_candy15 <- pivot_prep_candy15 %>% 
    pivot_longer(cols = 4:124, 
                 names_to = "candy", 
                 values_to = "reaction")
```
I continued programming this to remove all self identified non-edible variables including medication. The list can be seen in the `cleaning_script_task4_halloween_candy.csv`

### Cleaning `boing-boing-candy-2016.xlsx`

Again a similar process to the above with slight changes in order to facilitate the merging of all data sets later.
```{r, eval = FALSE}
pivot_prep_candy16 <- bbcandy2016 %>%
  select(-timestamp) %>% 
  rename(going_out  = are_you_going_actually_going_trick_or_treating_yourself,
         age        = how_old_are_you,
         gender     = your_gender,
         country    = which_country_do_you_live_in,
         province   = which_state_province_county_do_you_live_in) %>%   
  add_column(year = 2016, .before = 1) 
```
The pivot of the data had the same process as 2015's pivot code, however it had additional values I wanted to remove such as "person_of_interest_season_3_dvd_box_set_not_including_disc_4_with_hilarious_outtakes".

### Cleaning `boing-boing-candy-2017.xlsx`

For 2017 data, I noticed that there had been a prefix of "Q" then relative numbers to the variables, with "Q6" repeating itself. The below code removes that on all variables, using regex to look for a lowercase q at the beginning of the name, followed by a number and any amount of characters in addition to an underscore:

```{r, eval = FALSE}
names(bbcandy2017) = gsub(pattern = "^q[0-9]*_", replacement = "", 
                          x = names(bbcandy2017))
```

I then used THE same process as for the other years to prepare data set for pivot. I also renamed "anonymous_brown...." to mary_janes in order for that data to match with the other tables to give more accurate analysis.
```{r, eval = FALSE}
pivot_prep_candy17 <- bbcandy2017 %>%
  select(-c(internal_id, 110:120)) %>% 
  rename(province    = state_province_county_etc,
         mary_janes  = anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a_k_a_mary_janes) %>%   
  add_column(year = 2017, .before = 1) 
```

### Combining cleaned data sets and further cleaning

When I had combined them, I realised that I had missed some duplicates and errors. The below code enables more standardisation and more accurate results. I also used this oppurtunity to re-order the columns for ease of use.
```{r, eval = FALSE}
# joining all three cleaned datasets
candy_combined <- bind_rows(pivot_candy15, 
                            pivot_candy16, 
                            pivot_candy17) %>% 
  select(1,3,6,2,7,8, everything())

# cleaning further spelling errors for standardisation 
candy_combined <- candy_combined%>% 
  mutate(candy = case_when(
    candy == "boxo_raisins" ~ "box_o_raisins",
    candy == "sweetums_a_friend_to_diabetes" ~ "sweetums",
    candy == "anonymous_brown_globs_that_come_in_black_and_orange_wrappers" ~ "mary_janes",
    TRUE ~ candy)) 
```

I then had to create a pattern to identify the United Kingdom and the United States of America and replace them with a desired value. This was achieved by grouping the countries and extracting them as a list using `pull()`. I then had to delete countries in my list that I believed did not identify as the United States of America, and separate them with the `|` logical operator. I used this process for the United Kingdom also. Some specific country values were either not caught, or caught in a "usa" or "uk" process, therefore manual placement had to be made.

I then used the below code to carry out the "re-coding":
```{r, eval = FALSE}
# lowering case of country, recoding uk, usa, canada and other data country data only
candy_combined <-  candy_combined %>% 
  mutate(country = str_to_lower(country)) %>% 
  mutate(country = case_when(
    str_detect(country, "not[\\s]{1,}") ~ "other",
    str_detect(country, "australia") ~ "other",
    str_detect(country, "austria") ~ "other",
    str_detect(country, "soviet canuckistan") ~ "other",
    str_detect(country, "subscribe to dm4uz3 on youtube") ~ "other",
    str_detect(country, str_c(usa_pattern, collapse = "|")) ~ "usa",
    str_detect(country, str_c(uk_pattern, collapse = "|")) ~ "uk",
    str_detect(country, "^can") ~ "canada",
    is.na(country) == TRUE ~ "other",
    TRUE ~ "other")
  ) 
```

Lastly, the age column had to be cleaned as it was in character format and also held unrealistic ages of 0 years and 500+ years old.
```{r, eval = FALSE}
# cleaning age column, turning into integer for manipulation 
# using data only from 4 to 100 year olds
candy_combined <- candy_combined %>%
  mutate(age = as.integer(age)) %>% 
  mutate(age = ifelse(age <= 100, age, NA_integer_)) %>% 
  mutate(age = ifelse(age >= 4, age, NA_integer_)) %>% 
  drop_na(age) 
```


# Analysis

---The answers to the questions presented in the task brief

# Conclusions

---Any other interesting analyses or conclusions you come across.
